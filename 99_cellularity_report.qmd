---
title: "Report: Automation of cellularity calculation"
author: "William Midgley & Arron Lacey"
date: today
format: 
  html:
    code-fold: TRUE
    code-tools: TRUE
---
```{r}
#| include: false

setwd("C:\\Users\\william.midgley\\OneDrive - Swansea University\\Documents\\projects\\Amy's PhD\\cell-analysis")
library(tidyverse)
library(stringr)
library(knitr)
library(EBImage)
library(plot3D)

options(knitr.kable.NA = '')


blur <- 1.464

cut_off <- 0.07

error_factor <- 1.5

brightness_mean <- 0.3

grid_no <- 4
```
*See github and kaggle pages for code:*

- <https://github.com/whmidgley/cell-analysis>
- <https://www.kaggle.com/code/williammidgley/deep-learning-cellularity>
- <https://www.kaggle.com/code/williammidgley/grid-search-cellularity>

*These have not yet been made public*

## Introduction

Cellularity is a measure of general cell population. This is measured as the percentage of a brightfield image that is covered by cells (Application of this measure is covered in "Image Analysis Overview").

### Current method

Currently, images are taken using the brightfield microscope and ported into the proprietory image analysis software "Leica LAS AF", from which a researcher would look at each image and circle all cellular area (or acellular area depending on which is bigger) using a polygon tool within leica (fig.1). These polygons are saved as a (leica) .roi file which can then be used to only look at the pixels circled. This is done by creating a .csv file with a chart of all possible greyscale pixel brightnesses (0-255), and for each roi (circled area), the number of pixels with each given brightness is listed. The final cellularity is calculated by summing of all pixels in this chart and dividing by the total pixel number of the image. If acellular area is circled, the resulating proportion is taken from 1 to find the cellular area. This is done using excel functions. Furthermore, it is not possible to separate the two channels (gfp and brightfield) in this chart using this software, but since the rois overlay both channels, the cellularity is simply divided by 2.

![Fig.1: Example image with cellular area circled](report/Example roi image.png)

![Fig.2: Example pixel chart. Leftmost column shows pixel brightness, every two columns after represents a roi](report/Example chart 0.png)

## Automation

This process is extremely time consuming and for hundreds of images can take months. Furthermore, humans are prone to overestimate cellular area since small gaps in cells can often be missed, as well as general human error.
Fig.3 comapres the cellularities of a sample of images that were calculated by two different researchers (Gail and Amy). The two different researchers produced very similar but distinctly different results. The red line represents x=y where the cellularities are the same.
This helps demonstrate the uncertainty of cellularity calculated by a human. This will provide a useful baseline for comparison when testing accuracy of cellularity calculated automatically.

```{r}
gail_cells <- read.csv("gail_cellularities.csv")
amy_cells <- read.csv("amy_cellularities.csv")
gva <- cbind(gail_cells, amy_cells)


ggplot(data = gva) +
  aes(x = gail_cells, y = amy_cells) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, colour = "red") +
  ggtitle("Scatter plot comparing Amy's and Gail's cellularities") +
  xlab("Gail's cellularities (%)") +
  ylab("Amy's cellularities (%)")
```
Fig.3: a scatter plot comparing Amy's and Gail's cellularities

### Automating human calculated cellularities

In order to validate any algorithm that calculates cellularity, it must be compared to a number of human calculated cellularities on a wide range of images due to the number of artifacts and varying noise on brightfield images.
Since it would take a great deal of time to calculate the cellularities from the pixel charts for all validation images, we wrote a script in the programming language R that takes each chart and calculates cellularity from it outputting a .csv file of cellularities from all human image roi reports provided. This script can handle both cellular and acellular charts by using the naming conventions (ROI-cellular) and (ROI-acellular) in the file names.

### Automated cellularity algorithm

The next step is to perform image processing:

#### Image normalisation

These images often have gradients of light to dark over the image (fig.3). Therefore, we performed 3D linear regression over the image to find the general trend of brightness, and then we subtracted this gradient, and added 0.5 from the image to normalise it.

Fig.4: Original image:
```{r}
m_bf <- suppressWarnings(readImage("report/bf_example.tif"))

plot(m_bf)
```

Fig.5: 3D plot of image:

```{r}
persp3D(z = t(m_bf[,,1]), xlab = "right", ylab = "bottom", zlab = "brightness", theta = 120, phi = 40)

```

Fig.6a: 3D plane
```{r}
invisible(capture.output(source("01a_remove_gradient.r")))

d_normal <- data.frame(matrix(t(m_bf[,,1]), ncol=1))

  d_normal <-
  expand.grid(1:nrow(m_bf[,,1]), 1:ncol(m_bf[,,1])) %>%
  data.frame() %>%
  cbind(d_normal)

  colnames(d_normal) <- c("x", "y", "z")

  fit_normal <- lm(z ~ x + y, data = d_normal)

  if (is.na(fit_normal$coefficients[1])) {
    fit_normal$coefficients[1] <- 0
  }

  if (is.na(fit_normal$coefficients[2])) {
    fit_normal$coefficients[2] <- 0
  }

  if (is.na(fit_normal$coefficients[3])) {
    fit_normal$coefficients[3] <- 0
  }

  plane_normal <- outer(rows,cols,fit_normal,FUN=fun)

persp3D(z = plane_normal, xlab = "right", ylab = "bottom", zlab = "brightness", theta = 120, phi = 40)
```
Fig.6b: Plane as image
```{r}
plot(Image(plane_normal))
```

Fig.7: Normalised image
```{r}
plot(m_bf_normal)

```
Sometimes, this doesn't remove all gradients as there can be overlapping gradients. Therefore we performed this normalisation process three times per image.

#### Edge detection and blurring

We then used a sobel edge detector kernel (see below) to detect the edges of the image (Fig.8a). We used sobel since it is more computationally efficient and edge precision isn't absolutely necessary for this project

```{r}
#| include: false
source("01b_detect_edges.r")
```
```{r}
cat("Table: Vertical sobel kernel")
print(hfilt)
cat("Table: Horizontal sobel kernel")
print(vfilt)
```

These images all have horizontal artifacts running accross the images. Therefore, to help reduce the interferrence of these, we weighted the horizontal edges twice as much as the vertical edges, thereby keeping cell edges intact, but reducing the effect of the horizontal artifacts.

Fig.8a: Image edges
```{r}
plot(imgE)
```

We then used a gaussian blur to blur the edges (Fig.8b). This helps join the cell edges together to form a cellular region. The amount of blur was important since we wanted to ensure that the centres of the cells are filled, whilst reducing the expansion of cellular area due to blurring as much as possible.

Fig.8b: Blurred edge detected image
```{r}
plot(xb)
```

There is great variation within these images. Some images have very distinct cellular versus acellular regions, whereas on some images, it is hard to distinguish between the two. Therefore, we brightened the images by cutting off the brightest pixels and normalising the rest of the pixels back to 0-1 to help increase detection of cells. We only did this until the cellular area (which we could mostly tell from a cutoff) had a mean brightness of 0.3.

#### Applying a cut off

At this point, cellular area (which has a lot of edges in) is generally bright and acellular area (which is more smooth) is generally dark. Therefore, we applied a cut-off where any pixel with a brightness of below a certain cut-off is considered 0 (acellular) and any number above said cut-off is considerred 1 (cellular). The exact cut-off chosen needs to balance being high enough to include dark cellular area (fairly smooth cells), whilst removing as much noise as possible.

```{r}
#| include: false
source("01c_cut_off.r")
```
Fig.9a: Image after cut-off applied
```{r}
plot(m_bf_cut_off)
```

As you can see in Fig.9a, despite cellular area being well defined, there are small spots of noise remaining. Since these are all smaller than the size of an individual cell, we can simply remove these based on their size, producing Fig.9b.

Fig.9b: Removed artifacts of noise from image
```{r}
plot(m_bf_segmented)
```

Fig.9b is the final processed image, however if we overlay the outline of the image over the original image (see Fig.9c), you will notice there is a slight halo effect. This will lead to a slight overestimation of cellularity.

Fig.9c: outline of segmented image over original
```{r}
plot(m_bf_overlay) 
```

In order to mitigate against this overestimation, we subtracted a function of the total edges of the segmented area from the total cellular area. We used a function of the segmented area edge since the overestimation occurs at the cellular-acellular border. Therefore, an image with lots of cellular-acellular borders (like in the example image we have used throughout this report) will have more error due to this overestimation than an image where all the cells are clumped together on one side, which in turn will have more error than an image which is completely full or devoid of cells.

Once we have the total cellular area, we take the sum of the pixel values in the segmented image (where cellular pixels have the value 1 and acellular pixels have the value 0), and then divide this by the total number of pixels in the image to get the proportion of cellular area and times by 100 to get the percentage, i.e. cellularity.

For this image:
```{r}
cat("Cellularity is", round(computer_cellularity, 1),"%\n")
```

#### Applying a grid

It is useful to be able to calculate cellularities on certain sections of the images in a grid. Therefore we wrote a script that also gives cellularities in a grid of a size of the researcher's choice. In this example we have set it to a 4x4 grid.

```{r}
#| include: false
source("01d_by_grid.r")
```
```{r}
kable(m_cellularity_grid, col.names = NULL)
```
Table 1: A table showing the cellularities of a 4x4 grid over the example image

```{r}
plot(m_bf_overlay_grid)
```
Fig.10: Image with grid over it

### Validation

Table 2 shows the mean error, mean absolute error, and the mean squared error by comparing the automated cellularities to the human calculated ones 

Fig.10a is a scatterplot plotting human cellularities against automated cellularities. The red y=x straight line represents a perfect algorithm (assuming human cellularities are correct).
Fig.10b shows the variation in human calculated cellularities by plotting cellularities calculated by a different researcher, Gail, against Amy's cellularities and automated cellularities (similar to Fig.1 but including automated results). This shows that the variation between humans isn't that different to the variation between automation and humans.

Table 3 shows the mean error, mean absolute error, and the mean squared error of Amy's cellularities compared to Gail's cellularities. 

```{r}
auto_cells <- read.csv("automated_cellularities.csv")
human_cells <- read.csv("human_cellularities_fixed.csv")
auto_cells$name <- str_replace(auto_cells$name, "Effectene.lif_", "") %>%
  str_replace(".lif_", " ") %>%
  str_replace("Snapshot1.tif", "")
human_cells <- human_cells[,c(1,2)]

cells <- inner_join(auto_cells, human_cells, by = c("name" = "human_name"))
cells <- cells %>% mutate(
  error = cellularity - human_cellularity,
  )

tribble(~"mean error", ~"mean absolute error", ~"mean squared error",
  paste0(round(mean(cells$error), 2), "%"), paste0(round(mean(abs(cells$error)), 2), "%"), paste0(round(mean(cells$error^2), 2), "sq%")
  ) %>% kable()
```
Table 2: table showing mean error, mean absolute error, and the mean squared error of automated cellularities compared to human calculated cellularities

```{r}

ggplot(data = cells) +
  aes(x = human_cellularity, y = cellularity) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, colour = "red") +
  ggtitle("Scatter plot of human cellularities against automated cellularities") +
  xlab("human cellularities (%)") +
  ylab("automated cellularities (%)")

```
Fig.10a: plot of automated cellularities against human cellularities. Red line is x=y

```{r}
gail_auto_cells <- auto_cells[str_detect(auto_cells$name, "(20190531)|(20191114)"),]
gail_auto_cells <- cbind(gail_auto_cells[,2], "Automated")
amy_cells <- cbind(amy_cells, "Amy")
colnames(gail_auto_cells) <- c("cellularity", "auto_v_human")
colnames(amy_cells) <- c("cellularity", "auto_v_human")
colnames(gail_cells) <- "gail_cellularity"
gail_test <- cbind(rbind(gail_cells, gail_cells), rbind(gail_auto_cells, amy_cells))


ggplot(data = gail_test) +
  aes(x = gail_cellularity, y = as.numeric(cellularity), colour = auto_v_human) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0) +
  ggtitle("Scatter plot comparing Amy's and automated cellularities against Gail's") +
  xlab("Gail's cellularities (%)") +
  ylab("Amy's and automated cellularities (%)") +
  scale_colour_discrete(name = "")

```
Fig.10b: plot of Gail's cellularities against Amy's cellularities and automated cellularities. Black line is x=y

```{r}
amy_error <- cbind(gail_cells, amy_cells[,1])
colnames(amy_error) <- c("gail", "amy")
amy_error$error <- amy_error$amy - amy_error$gail

auto_error <- cbind(gail_cells, as.numeric(gail_auto_cells[,1]))
colnames(auto_error) <- c("gail", "auto")
auto_error$error <- auto_error$auto - auto_error$gail


tribble(~"Cellularity", ~"mean error", ~"mean absolute error", ~"mean squared error",
  "Amy", paste0(round(mean(amy_error$error), 2), "%"), paste0(round(mean(abs(amy_error$error)), 2), "%"), paste0(round(mean(amy_error$error^2), 2), "sq%"),
  "Automated", paste0(round(mean(auto_error$error), 2), "%"), paste0(round(mean(abs(auto_error$error)), 2), "%"), paste0(round(mean(auto_error$error^2), 2), "sq%")
  ) %>% kable()
```
Table 3: table showing the mean error, mean absolute error, and the mean squared error of Amy's and automated cellularities compared to Gail's cellularities

#### Gradient descent and gridsearch

In this algorithm there are three main variables that affect the accuracy of the model: blur, cut-off, and error factor (i.e. the proportion of the segmented image edges that we subtract from the segmented cellular area. 

### Limitations

Some objects (e.g. bubbles or imaging artifacts) are sometimes counted as cellular area. Fortunatly, most bubbles are relatively large if they do exist, meaning that the blurring that we apply is not sufficient to cover the whole bubble. However, this does still sometimes cause cellularity to be slightly overestimated although the effect of this is largely minimal on the end cellularity.

### Other techniques considered and tested

#### K-means clustering

The first method we tried was using k-means clustering which is an unsupervised learning technique. This means that it finds the nearest centroids (cluster centre) to a given number of groups of similar datapoints by itself. In this context, it takes all the different shades and groups them into their nearest cluster, forming a segmented image.
Using k-means before edge detection was unsuitable since similar shades occur both in cellular and acellular area, and often background noise would cause acellular area to be segmented also.
The major difference between cellular and acellular area from an image processing perspective is that acellular area is (more) consistant (I have discussed the background noise in the edge detection section) wheras cellular area has many edges. Each cell has a dark outer ring and a bright centre. Therefore, we decided to apply edge detection. We then applied blurring for reasons previously explained.
From here, we used k-means clustering on the blurred, edge detected image and took the area of the darkest cluster. This worked as a good start, but it makes the assumption that there is enough dark (acellular) area in the image that it will form its own cluster. For images with a high cellularity, the darkest cluster is still cellular area.
From there we clustered the images into clusters determined by a representitative sample. However, since we are using consistent clusters, and we're only interested in the darkest cluster, this is indistinguishable from a simple cut-off. Therefore we used the cut-off as a less computationally intensive alternative.

#### Deep learning

We also attempted convolutional neural networks (CNNs) to calculate cellularity. We used the human calculated cellularities (outcome variable) and their respective images (predictor variable) as the training dataset. We tried an linear final activation function, and a sigmoid final activation function (these are different ways of outputting the results of the CNN). Because of our very small dataset for CNNs (~100 images), our model could not attain a competitive accuracy. In order to mitigate against this, we used transfer learning (adapting pretrained models by training only the last few layers on our own dataset) and image augmentation to increase accuracy (flipping the image and changing the brightness to provide more training data and reduce overfitting). This made the model much more competitive, achieving a mean error of 3-4%, however this is still less accurate than the edge detection method.

There is potential for the use of deep learning, however with the size of the training datasets available to us it is unlikely to significantly improve on the edge detection approach. Furthermore, using the edge detection approch, we are able to validate the outputs of the algorithm by simply looking at the segmented image. This contrasts the CNN method where there is only one cellularity output with no segmented image. One way to combat this is to train the model on segmented images as opposed to raw cellularities. The reason we did not do this is because there are no leica .roi reader R packages. We attempted parsing the files but could not find a suitable method.

